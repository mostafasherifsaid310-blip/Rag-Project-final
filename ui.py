import gradio as gr
from fastapi import FastAPI, UploadFile, File
from typing import List
import threading

from ingestion import upload_files
from chat_hybrid import hybrid_chat

conversation_history = []

# -------------------- FastAPI --------------------
app = FastAPI(title="âš¡ Nitro Hybrid Chat API")

@app.post("/upload")
async def upload(files: List[UploadFile] = File(...)):
    texts = []
    for f in files:
        content = await f.read()
        texts.append(f"{f.filename}: {len(content)} bytes")
    return {"uploaded": texts}

@app.post("/chat")
async def chat_api(message: str):
    global conversation_history
    conversation_history.append({"role": "user", "content": message})

    full_answer = ""
    for chunk in hybrid_chat(message):
        full_answer += chunk  # Ù†Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù†ØµÙˆØµ

    conversation_history.append({"role": "assistant", "content": full_answer})
    return {"answer": full_answer, "conversation": conversation_history}

@app.post("/clear")
def clear_api():
    global conversation_history
    conversation_history = []
    return {"status": "cleared"}

@app.get("/")
def root():
    return {"message": "âš¡ Nitro Hybrid Chat API is running!"}

@app.get("/favicon.ico")
def favicon():
    return ""
# -------------------- Gradio --------------------
def chat_gradio(message, history):
    history = history or []

    conversation_history.append({"role": "user", "content": message})
    conversation_history.append({"role": "assistant", "content": ""})

    for partial_answer in hybrid_chat(message):
        conversation_history[-1]["content"] = partial_answer
        yield (
            [{"role": msg["role"], "content": msg["content"]} for msg in conversation_history],
            ""
        )
        
        
    conversation_history[-1]["content"] = partial_answer
    yield (
            [{"role": msg["role"], "content": msg["content"]} for msg in conversation_history],
            ""
        )

def clear_gradio():
    global conversation_history
    conversation_history = []
    try:
        from memory import conversation_history as memory_history
        memory_history.clear()
    except ImportError:
        pass
    return []

def run_gradio():
    with gr.Blocks() as demo:
        gr.Markdown("# âš¡Nitro")

        with gr.Tab("ğŸ“‚ Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª"):
            file_input = gr.File(file_types=[".pdf", ".docx"], file_count="multiple")
            upload_btn = gr.Button("Upload")
            upload_output = gr.Textbox(label="Ù†ØµÙˆØµ Ø§Ù„Ù…Ù„ÙØ§Øª")

            upload_btn.click(upload_files, file_input, upload_output)

        with gr.Tab("ğŸ’¬ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©"):
            chatbot = gr.Chatbot(label="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯")
            msg = gr.Textbox(placeholder="Ø§ÙƒØªØ¨ Ø±Ø³Ø§Ù„ØªÙƒ Ù‡Ù†Ø§ Ø«Ù… Ø§Ø¶ØºØ· Enter...")
            clear = gr.Button("Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")

            msg.submit(chat_gradio, [msg, chatbot], [chatbot, msg])
            clear.click(clear_gradio, None, chatbot)
            
        demo.queue()
        demo.launch(server_name="127.0.0.1", server_port=7860,share=False)
       

# -------------------- ØªØ´ØºÙŠÙ„ FastAPI + Gradio Ù…Ø¹Ù‹Ø§ --------------------
if __name__ == "__main__":
    import uvicorn
    import time

    # Ù†Ø¨Ø¯Ø£ Gradio ÙÙŠ Thread Ù…Ù†ÙØµÙ„
    t = threading.Thread(target=run_gradio, daemon=True)
    t.start()

    # Ù†Ù†ØªØ¸Ø± Ø«Ø§Ù†ÙŠØ© ØµØºÙŠØ±Ø© Ù‚Ø¨Ù„ ØªØ´ØºÙŠÙ„ FastAPI Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Gradio Ø¨Ø¯Ø£
    time.sleep(1)

    # ØªØ´ØºÙŠÙ„ FastAPI Ø¹Ù„Ù‰ Ø¨ÙˆØ±Øª Ù…Ø®ØªÙ„Ù Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªØ¹Ø§Ø±Ø¶
    uvicorn.run(app, host="127.0.0.1", port=8000)


