import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from hf_api import query_hf_chat_model_stream

DB_PATH = "vectordb"

embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=600,
    chunk_overlap=80
)

vectordb = None


def load_vectorstore():
    global vectordb
    if os.path.exists(DB_PATH):
        vectordb = FAISS.load_local(
            DB_PATH,
            embedding_model,
            allow_dangerous_deserialization=True
        )


def build_vectorstore(text):
    global vectordb

    chunks = splitter.split_text(text)

    vectordb = FAISS.from_texts(
        texts=chunks,
        embedding=embedding_model
    )

    vectordb.save_local(DB_PATH)


def retrieve_docs(query, k=3):
    if vectordb is None:
        return []

    return vectordb.similarity_search(query, k=k)


# def rag_with_citations(question):

#     docs = retrieve_docs(question)

#     if not docs:
#         return None

#     context = "\n".join([doc.page_content for doc in docs])

#     prompt = f"""
# Ø£Ø¬Ø¨ Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Ù‹ ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:

# {context}

# Ø§Ù„Ø³Ø¤Ø§Ù„:
# {question}

# Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨ÙˆØ¶ÙˆØ­ Ù‚Ù„:
# "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª"
# """

#     full_answer = ""

#     # ğŸ”¥ Streaming Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
#     for partial_answer in query_hf_chat_model_stream(prompt):
#         full_answer = partial_answer
#         yield full_answer

#     # Ù„Ùˆ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù‚Ø§Ù„ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© â†’ Ù†Ø±Ø¬Ø¹ None
#     if "ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©" in full_answer:
#         return

#     citations = "\n\nğŸ“š Sources:\n"
#     for i, doc in enumerate(docs, 1):
#         citations += f"[{i}] {doc.page_content[:120]}...\n"

#     yield full_answer + citations



def rag_with_citations(question):

    docs = retrieve_docs(question)

    if not docs:
        return None

    context = ""
    sources = []

    for doc in docs:
        context += doc.page_content + "\n\n"
        sources.append(doc.page_content[:120])

    prompt = f"""
Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª.

Ù„Ø¯ÙŠÙƒ Ù…Ù‚ØªØ·ÙØ§Øª Ù…Ù† Ù…Ù„ÙØ§Øª Ù…Ø®ØªÙ„ÙØ©.

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:

- Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ù„Ù
- Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù‡Ù†Ø§Ùƒ Ø¹Ø¯Ø© Ù…ØµØ§Ø¯Ø± â†’ Ù‚Ù… Ø¨Ø§Ù„Ø¯Ù…Ø¬ ÙˆØ§Ù„ØªØ­Ù„ÙŠÙ„
- Ø¥Ø°Ø§ Ø·ÙÙ„Ø¨Øª Ù…Ù‚Ø§Ø±Ù†Ø© â†’ ÙˆØ¶Ù‘Ø­ Ø§Ù„ÙØ±ÙˆÙ‚Ø§Øª Ø¨ÙˆØ¶ÙˆØ­
- Ù„Ø§ ØªØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†Øµ ÙƒÙ…ØµØ¯Ø± ÙˆØ§Ø­Ø¯
- Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ØªØ±Ø§Ø¨Ø·Ø©

Ø§Ù„Ù…Ø­ØªÙˆÙ‰:
{context}

Ø§Ù„Ø³Ø¤Ø§Ù„:
{question}
"""

    full_answer = ""

    for partial in query_hf_chat_model_stream(prompt):
        full_answer = partial
        yield full_answer

    citations = "\n\nğŸ“š Sources:\n"
    for i, src in enumerate(sources, 1):
        citations += f"[{i}] {src}...\n"

    yield full_answer + citations



load_vectorstore()
